#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import re
import time
import random
import urllib.parse
import pandas as pd
import requests
from bs4 import BeautifulSoup
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import json
from fake_useragent import UserAgent
from requests.adapters import HTTPAdapter
from datetime import datetime

DRIVE_CSV_PATH = "coupang_all_categories.csv"

def load_categories_from_txt(filename="categories.txt"):
    categories = {}
    with open(filename, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            if "," in line:
                industry, keyword = line.split(",", 1)
                industry = industry.strip().lower()
                keyword = keyword.strip().lower()
                categories.setdefault(industry, []).append(keyword)
            else:
                categories.setdefault("ê¸°íƒ€", []).append(line.strip().lower())
    return categories

class CoupangCrawler:
    def __init__(self, min_per_industry=10, categories_file="categories.txt"):
        self.categories_dict = load_categories_from_txt(categories_file)
        self.min_per_industry = min_per_industry
        self.df_total = pd.DataFrame(columns=[
            "í‚¤ì›Œë“œ", "ì—…ì¢…", "ëª¨ë¸ëª…", "ê°€ê²©", "ìƒí’ˆìƒíƒœ",
            "ê²Œì‹œì¼", "ì¡°íšŒìˆ˜", "ì¢‹ì•„ìš”ìˆ˜", "íŒë§¤ì™„ë£Œì—¬ë¶€",
            "ë°°ì†¡ê°€ëŠ¥", "í’ˆì§ˆë“±ê¸‰"
        ])
        self.session = requests.Session()
        adapter = HTTPAdapter(pool_connections=100, pool_maxsize=100)
        self.session.mount("http://", adapter)
        self.session.mount("http://", adapter)

        self.processed_urls = set()
        self.df_lock = threading.Lock()
        self.url_lock = threading.Lock()
        self.ua = UserAgent()
        self.retry_limit = 3

        self.request_count = 0
        self.sleep_threshold = 600 
        self.big_sleep_time = 600 

    def _get_random_delay(self):
        return random.uniform(1, 3)

    def _rotate_user_agent(self):
        return self.ua.random

    def _crawl_item_detail(self, product_url: str) -> dict:
        for attempt in range(self.retry_limit):
            try:
                self.request_count += 1

                if self.request_count % self.sleep_threshold == 0:
                    print(f"{self.big_sleep_time}ì´ˆ ë™ì•ˆ ëŒ€ê¸° (ì°¨ë‹¨ ë°©ì§€)")
                    time.sleep(self.big_sleep_time)

                headers = {
                    "User-Agent": self._rotate_user_agent(),
                    "Referer": "http://www.coupang.com",
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
                }
                full_url = urllib.parse.urljoin("http://www.coupang.com", product_url)
                resp = self.session.get(full_url, headers=headers, timeout=10)

                if resp.status_code == 403:
                    print(f"ğŸš¨ ì°¨ë‹¨ ê°ì§€(403)! 10ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(10)
                    continue
                elif resp.status_code == 404:
                    print(f"ğŸš¨ 404 Not Found! 10ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(10)
                    continue

                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, "html.parser")

                title_tag = soup.find("h2", class_="prod-buy-header__title")
                if not title_tag:
                    title_tag = soup.find("h2", class_="prod-title")
                title = title_tag.get_text(strip=True) if title_tag else "ì œëª©ì—†ìŒ"

                price_tag = soup.find("span", class_="total-price")
                if price_tag:
                    price_text = price_tag.get_text(strip=True)
                    price = re.sub(r"[^\d]", "", price_text)
                else:
                    price = None

                sold_out = False
                if soup.find(string=re.compile("í’ˆì ˆ")):
                    sold_out = True
                condition = "í’ˆì ˆ" if sold_out else "ìƒˆìƒí’ˆ"

                posted_str = "ì•Œìˆ˜ì—†ìŒ"
                view_count = "ì•Œìˆ˜ì—†ìŒ"
                like_count = 0
                delivery = not sold_out
                quality = 3

                return {
                    "ëª¨ë¸ëª…": title,
                    "ê°€ê²©": price,
                    "ìƒí’ˆìƒíƒœ": condition,
                    "ê²Œì‹œì¼": posted_str,
                    "ì¡°íšŒìˆ˜": view_count,
                    "ì¢‹ì•„ìš”ìˆ˜": like_count,
                    "íŒë§¤ì™„ë£Œì—¬ë¶€": sold_out,
                    "ë°°ì†¡ê°€ëŠ¥": delivery,
                    "í’ˆì§ˆë“±ê¸‰": quality
                }
            except Exception as e:
                print(f"ğŸš¨ ìƒì„¸ URL={product_url} ì‹œë„ {attempt+1}/{self.retry_limit} ì‹¤íŒ¨: {e}")
                time.sleep(self._get_random_delay() * (attempt + 1))

        return {
            "ëª¨ë¸ëª…": "ì œëª©ì—†ìŒ",
            "ê°€ê²©": None,
            "ìƒí’ˆìƒíƒœ": "ì•Œìˆ˜ì—†ìŒ",
            "ê²Œì‹œì¼": "ì•Œìˆ˜ì—†ìŒ",
            "ì¡°íšŒìˆ˜": "ì•Œìˆ˜ì—†ìŒ",
            "ì¢‹ì•„ìš”ìˆ˜": 0,
            "íŒë§¤ì™„ë£Œì—¬ë¶€": False,
            "ë°°ì†¡ê°€ëŠ¥": False,
            "í’ˆì§ˆë“±ê¸‰": 3
        }

    def _process_item(self, item, keyword: str, industry: str):
        product_url = item.get("url")
        with self.url_lock:
            if product_url in self.processed_urls:
                return False
            self.processed_urls.add(product_url)

        with self.df_lock:
            if len(self.df_total[self.df_total['ì—…ì¢…'] == industry]) >= self.min_per_industry:
                return False

        detail_data = self._crawl_item_detail(product_url)

        time.sleep(self._get_random_delay())

        new_row = {"í‚¤ì›Œë“œ": keyword, "ì—…ì¢…": industry}
        new_row.update(detail_data)

        with self.df_lock:
            self.df_total = pd.concat([self.df_total, pd.DataFrame([new_row])], ignore_index=True)

        return True

    def _crawl_keyword(self, keyword: str, industry: str, target_count: int):
        max_page = 10
        page = 1

        while page <= max_page:
            with self.df_lock:
                if len(self.df_total[self.df_total['ì—…ì¢…'] == industry]) >= self.min_per_industry:
                    break

            try:
                self.request_count += 1

                if self.request_count % self.sleep_threshold == 0:
                    print(f"{self.big_sleep_time}ì´ˆ ë™ì•ˆ ëŒ€ê¸° (ì°¨ë‹¨ ë°©ì§€)")
                    time.sleep(self.big_sleep_time)

                search_url = f"http://www.coupang.com/np/search?component=&q={urllib.parse.quote(keyword)}&page={page}"
                headers = {
                    "User-Agent": self._rotate_user_agent(),
                    "Referer": "http://www.coupang.com",
                }
                resp = self.session.get(search_url, headers=headers, timeout=10000)

                if resp.status_code == 403:
                    print(f"ğŸš¨ ì°¨ë‹¨ ê°ì§€(403)! 10ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(10)
                    continue
                elif resp.status_code == 404:
                    print(f"ğŸš¨ 404 Not Found! 10ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                    time.sleep(10)
                    continue

                resp.raise_for_status()
                soup = BeautifulSoup(resp.text, "html.parser")

                items = []
                for li in soup.find_all("li", class_=re.compile("search-product")):
                    a_tag = li.find("a", href=re.compile("^/vp/products/"))
                    if a_tag:
                        product_url = a_tag.get("href")
                        items.append({"url": product_url})

                if not items:
                    print(f"ğŸš¨ [{keyword}] í˜ì´ì§€ {page}ì—ì„œ ìƒí’ˆì„ ì°¾ì§€ ëª»í•¨")
                    break

                with ThreadPoolExecutor(max_workers=5) as executor:
                    futures = []
                    for item in items:
                        with self.df_lock:
                            if len(self.df_total[self.df_total['ì—…ì¢…'] == industry]) >= self.min_per_industry:
                                break
                        futures.append(executor.submit(self._process_item, item, keyword, industry))

                    for future in as_completed(futures):
                        _ = future.result()

                page += 1
                time.sleep(self._get_random_delay())

            except Exception as e:
                print(f"ğŸš¨ [{keyword}] í˜ì´ì§€ {page} ì˜¤ë¥˜: {e}")
                time.sleep(self._get_random_delay() * 2)
                continue

    def _crawl_industry(self, industry, keywords):
        for keyword in keywords:
            print(f"ğŸ” [{industry}] í‚¤ì›Œë“œ: {keyword}")
            self._crawl_keyword(keyword, industry, self.min_per_industry)
            print(f"âœ… [{industry}] '{keyword}' ìˆ˜ì§‘ ì™„ë£Œ: {len(self.df_total[self.df_total['ì—…ì¢…'] == industry])}ê±´")

    def run(self):
        print("ğŸš€ ì¿ íŒ¡ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        start_time = time.time()

        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = []
            for industry, keywords in self.categories_dict.items():
                futures.append(executor.submit(self._crawl_industry, industry, keywords))

            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"ğŸš¨ ì˜ˆì™¸ ë°œìƒ: {e}")

        elapsed = time.time() - start_time
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print(f"ğŸ“Š ì „ì²´ ìˆ˜ì§‘ ê±´ìˆ˜: {len(self.df_total)}ê±´")

    def save_csv(self, filename):
        self.df_total.to_csv(filename, index=False, encoding="utf-8-sig")
        print(f"ğŸ‰ ì €ì¥ ì™„ë£Œ: {filename}, ì´ {len(self.df_total)}ê±´")


if __name__ == "__main__":
    crawler = CoupangCrawler(min_per_industry=10, categories_file="categories.txt")
    crawler.run()
    crawler.save_csv(DRIVE_CSV_PATH)
